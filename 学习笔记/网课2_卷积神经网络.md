# 卷积神经网络

## 深度学习三部曲

1. **搭建神经网络结构**
2. **找到合适的损失函数**：交叉熵损失、均方误差
   - 损失函数是用来衡量真实值和预测值的吻合度的
3. **找到一个适合的优化函数，更新参数**：BP，随机梯度下降

## 基本组成结构【局部关联、参数共享、滑窗（使得其区别于全连接的传统神经网络）】

### 卷积 convolution layer 

- 定义：卷积是对两个实变函数的一种数学操作

$$
\vec{y}=a(W· \vec{x}+b)
$$

图像：$X∈R^{M*N}$

滤波器【卷积】： $W∈R^{m*n}$

- 基本概念：

  - 输入
  - kernel/filter 卷积核/滤波器
  - weights 权重
  - receptive field 感受野
  - activation map/feature map 特征图
  - stride 步长 
  - depth/channel 深度：与filter的个数一致。
  - padding
  - 输出

  ![](D:\WeChat\文件\WeChat Files\wxid_erjfuxl07n6m21\FileStorage\File\2025-02\Capture_20250226_153704.jpg)

- 输出的特征图的大小：

  $N$：为input的大小

  $F$：为kernel的大小

  1. 没有填充（padding）
     $$
     (N-F)/stride + 1
     $$
     
  2. padding（零填充）
     $$
     (N+padding*2-F)/stride + 1
     $$
  
- 例题：

  ![image-20250226151350874](C:\Users\Chen\AppData\Roaming\Typora\typora-user-images\image-20250226151350874.png)

### 池化 pooling layer【没有参数】

- 缩放过程，没有参数
- 作用：
  - 保留主要特征的同时减少参数和计算量，防止过拟合
  - 卷积层之间、全连接层之间
- 类型：（一般 filter：2*2、stride：2）
  - 最大值池化【更好一些】：寻找**感受野**中的最大值
  - 平均值池化：计算**感受野**中的平均值

### 全连接层 fully connected layer

1. 两层之间的所有神经元都有权重链接

2. 通常在尾部

3. 全连接参数量最大



------

## 卷积神经网络典型结构

### AlexNet【深度学习转折点】

- 模型结构：

  ![image-20250226194310512](C:\Users\Chen\AppData\Roaming\Typora\typora-user-images\image-20250226194310512.png)

- 成功的原因：

  - 大数据训练

  - 非线性激活函数：ReLU
    $$
    ReLu=max(0,x)
    $$

    - 收敛速度快
    - 没有梯度消失【正区间导数不为0】

  - 防止过拟合：

    - **Dropout**：随机失活，训练时随即关闭部分神经元，测试时整合所有的神经元
    - **Data augmentation**：通过 **平移、反转、对称** 操作，获得大量样本

### ZFNet

在**AlexNet**的基础上**减小步长和感受野的大小**

### VGG

- 是一个更深的网络：`8 layers(AlexNet) ----> 16-19(VGG)`

### GoogleNet

- 网络总体结构：

  - 包含22个带参数的层，没有额外的FC层（保留最后的类别输出层）
  - **inception模块不同**

- inception模块：**多卷积核增加特征多样性**

  - 每个卷积核的**深度不同，但大小相同**(padding)

  - 原始结构：计算复杂

    ![image-20250226195523684](C:\Users\Chen\AppData\Roaming\Typora\typora-user-images\image-20250226195523684.png)

  - v2: 插入**1*1卷积核**进行降维（channal减小）

    ![image-20250226195804150](C:\Users\Chen\AppData\Roaming\Typora\typora-user-images\image-20250226195804150.png)

  - v3：用小的卷积核代替大的卷积核

    ![image-20250226195904799](C:\Users\Chen\AppData\Roaming\Typora\typora-user-images\image-20250226195904799.png)

    - 降低参数量
    - 增加非线性激活函数：增加非线性激活函数使网络产生更多独立特，表征能力更强，训练更快

### ResNet

- 残差学习网络

![image-20250226202026808](C:\Users\Chen\AppData\Roaming\Typora\typora-user-images\image-20250226202026808.png)

- 优点：
  - 可以去掉相同的主体部分，从而突出微小的变化
  - 不会出现梯度消失