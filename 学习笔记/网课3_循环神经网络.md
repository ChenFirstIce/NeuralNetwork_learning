# 循环神经网络【RNN】

## 循环神经网络vs卷积神经网络

- 核心问题：上下文关系(时序)
- 目标：考虑更多的上下文

- 不同：
  - 传统神经网络，卷积神经网络，输入和输出之间使是相互独立的
  - RNN可以有更好的处理具有时序关系的任务
  - 引入“记忆”的概念

## RNN基本结构

- 基本结构：

![Capture_20250227_221748](D:\WeChat\文件\WeChat Files\wxid_erjfuxl07n6m21\FileStorage\File\2025-02\Capture_20250227_221748.jpg)

![Capture_20250227_221753](D:\WeChat\文件\WeChat Files\wxid_erjfuxl07n6m21\FileStorage\File\2025-02\Capture_20250227_221753.jpg)

- 优点：
  - f被不断地使用
  - 模型所需要学习的参数是固定的
  - 无论我们输入的长度是多少，我们只需要一个函数f

### 深度RNN

![Capture_20250227_221759](D:\WeChat\文件\WeChat Files\wxid_erjfuxl07n6m21\FileStorage\File\2025-02\Capture_20250227_221759.jpg)

### 双向RNN

![Capture_20250227_221803](D:\WeChat\文件\WeChat Files\wxid_erjfuxl07n6m21\FileStorage\File\2025-02\Capture_20250227_221803.jpg)

## BPTT算法

- RNN的基本公式：
  $$
  h_t=tan(Ux_t+Wh_{t-1})\\
  \hat{y_t} = softmax(Vh_t)\\
  E=\sum{E_t}
  $$
  损失函数（交叉熵）：
  $$
  E_t=-y_tlog(\hat{y_t})
  $$

- BPTT算法：

  <img src="C:\Users\Chen\AppData\Roaming\Typora\typora-user-images\image-20250227222540961.png" alt="image-20250227222540961" style="zoom:33%;" />

  

- 问题：tan' <= 1

  - 如果 W∈(0,1) ，会出现**梯度消失**
  - 如果 W > 1，会出现**梯度爆炸**

- 改进：

  - 权重衰减
  - 梯度截断

# LSTM【不会出现梯度消失的问题】

- 基本结构：

  - 遗忘门：决定$C_{t-1}$保留多少

    ​               保留概率：$f_t=\sigma(W_C·[h_{t-1},x]+b_C)$

  - 输入门：决定$C_t$中有多少$\hat{C_{t}}=tanh(W_C·[h_{t-1},x]+b_C)$

  ​                       保留概率：$i_t=\sigma(W_C·[h_{t-1},x]+b_C)$

  
  $$
  C_t=f_t*C_{t-1}+i_t*\hat{C_t}
  $$

  ------

  

  - 输出门：决定那个部分被输出出去。保留概率：$o_t=\sigma(W_C·[h_{t-1},x]+b_C)$
    - $h_t=o_t·tanh(C_t)$
    - $y_t = softmax(h_t)$

  ![image-20250228101631991](C:\Users\Chen\AppData\Roaming\Typora\typora-user-images\image-20250228101631991.png)

- 其他变形：
  - peewhole
  - 遗忘门和输入们链接：概率和为1

## 门控循环单元GRU

- 与LSTM的差异：
  - GRU只有两个门：重置门和更新门
  - $C_t$和$h_t$重合 

![image-20250228102910668](C:\Users\Chen\AppData\Roaming\Typora\typora-user-images\image-20250228102910668.png)

## Attention机制

- 每个时刻根据当前记忆学到一个attention权重矩阵，权重越大，注意力越强