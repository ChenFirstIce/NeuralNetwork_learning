# 循环神经网络【RNN】

## 循环神经网络vs卷积神经网络

- 核心问题：上下文关系(时序)
- 目标：考虑更多的上下文

- 不同：
  - 传统神经网络，卷积神经网络，输入和输出之间使是相互独立的
  - RNN可以有更好的处理具有时序关系的任务
  - 引入“记忆”的概念

## RNN基本结构

- 基本结构：

![Capture_20250227_221748](D:\WeChat\文件\WeChat Files\wxid_erjfuxl07n6m21\FileStorage\File\2025-02\Capture_20250227_221748.jpg)

![Capture_20250227_221753](D:\WeChat\文件\WeChat Files\wxid_erjfuxl07n6m21\FileStorage\File\2025-02\Capture_20250227_221753.jpg)

- 优点：
  - f被不断地使用
  - 模型所需要学习的参数是固定的
  - 无论我们输入的长度是多少，我们只需要一个函数f

### 深度RNN

![Capture_20250227_221759](D:\WeChat\文件\WeChat Files\wxid_erjfuxl07n6m21\FileStorage\File\2025-02\Capture_20250227_221759.jpg)

### 双向RNN

![Capture_20250227_221803](D:\WeChat\文件\WeChat Files\wxid_erjfuxl07n6m21\FileStorage\File\2025-02\Capture_20250227_221803.jpg)

## BPTT算法

- RNN的基本公式：
  $$
  h_t=tan(Ux_t+Wh_{t-1})\\
  \hat{y_t} = softmax(Vh_t)\\
  E=\sum{E_t}
  $$
  损失函数（交叉熵）：
  $$
  E_t=-y_tlog(\hat{y_t})
  $$

- BPTT算法：

  <img src="C:\Users\Chen\AppData\Roaming\Typora\typora-user-images\image-20250227222540961.png" alt="image-20250227222540961" style="zoom:33%;" />

  

- 问题：tan' <= 1

  - 如果 W∈(0,1) ，会出现**梯度消失**
  - 如果 W > 1，会出现**梯度爆炸**

- 改进：

  - 权重衰减
  - 梯度截断

# LSTM