# 神经网络基础

## 神经元特征

- 多个输入单个输出

- 空间整合
- 兴奋性/抑制性输入——权重w模拟兴奋/抑制
- 阈值特性





## 万有逼近定理【*所以是要暴力拼算力了吗？*下面有回答】

- 【更宽】定义：如果有一个隐层**包含足够多的神经元**，三层前神经网络（输入-隐层-输出）能以任意精度逼近人已预定的**连续函数**。

- 【更深】推广（双隐层感知器逼近非连续函数）：**当隐层足够宽时**，双隐层感知器（输入-隐层1-隐层2-输出）可以逼近任意**非连续函数**：可以解决任何复杂的分类问题。

- 理解：

  - 每一层的数学公式：
    $$
    \vec{y}=a(W· \vec{x}+b)
    $$

    - 线性变换（**W·x**）：升维/降维 ＆ 放大/缩小 & 旋转
    - 线性变化（+b）：平移
    - 非线性变换【a(·)】：弯曲

  - 作用：

    - 神经网络学习利用矩阵的线性变换加激活函数的非线性变换，**将原始输入空间投影到线性可分的空间**去分类/回归。
    - 增加节点数：增加维度，即增加线性转换能力。
    - 增加层数：增加激活函数的次数，即增加非线性转换次数。



更宽or更深：**更深**，瘦高的网络更好。



## 深层神经网络的问题：梯度消失

### 神经网络的参数学习：误差反向传播【本质：复合函数的链式求导】

- 三层前馈神经网络的BP算法：【前向传播和反向传播】

  - 误差：

    z=W·x+b

    a = σ(z)
    $$
    L=\sum_{k=1}^c(a_k-y_k)^2
    $$
    

    激活函数
    $$
    \sigma ={1\over{1+e^{-x}}}
    $$
    
- 前馈
  
- 残差：损失函数在某个节点的偏导

#### BP算法：

- 过程：分为前向传播和反向传播

  - 前向传播：

    从输入层→隐藏层→输出层，每一层（即做两次计算）都需要计算

  $$
  z=\begin{matrix}W\end{matrix}·\vec{x}+b
  $$

  然后使用一次激活函数。

  最终获得输出层的值，与真实值比较，计算损失函数（均方误差）L

  - 反向传播：

  损失函数L分别对上一层计算时的权重w和偏置b求偏导【使用链式求导法则】，使用**梯度下降法**更新权重和偏置的值



# 第三次兴起

## 逐层预训练

- BP算法缺点：
  - 局部极小值（并不是最优解）
  - 梯度消失
  - **解决方法**：找到一个较好的初始值
- **微调和逐层预训练**都可以改善最终的结果，但是**微调的效果较小**

## 逐层预训练的方法

### 自编码器

